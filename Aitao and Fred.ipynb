{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pyarabic.araby as araby\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"الإيمانُ ..وأثره على الصحة النفسية\n",
    "كان علم النفس دائماً فخوراً كعلم بالجامعات بتقاليده الدنيوية )غير\n",
    "الدينية( الملتزمة بالتنوير. وكان -على الدوام- من ضمن هذه\n",
    "التقاليد وجود شك واضح بكل أشكال التدين  كما يصف بيرنارد\n",
    "وغروم\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def removeStopWords(text):\n",
    "#     tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "#     sw = stopwords.words('arabic')\n",
    "\n",
    "#     tokens = [token for token in tokens if not token in sw]\n",
    "#     updated_text = ' '.join(tokens)\n",
    "#     return updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Read stop words from file\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stop_words = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = []\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "    updated_text = ' '.join(filtered_tokens)\n",
    "    return updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_declaration(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    new_words = []\n",
    "    for word in tokens:\n",
    "        if word.startswith(\"ال\"):\n",
    "            new_words.append(word[2:])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    updated_text = ' '.join(new_words)\n",
    "    return updated_text\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_declaration(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prefixes =  ['وليست', 'فليست', 'اليست', 'افلست',\n",
    "'اتست', 'ويست', 'فاست', 'واست', 'انهم', 'والم', 'باست', 'الاس', 'كمست', 'والا', 'فاست',\n",
    "'مست', 'ولت', 'فلي', 'فلن', 'فلل', 'فان', 'يست', 'است', 'تست', 'فلي', 'وسي', 'وسن', 'فلا', 'وست', 'بمس',\n",
    "'او', 'اي', 'ان', 'في', 'فب', 'فت', 'لي', 'فن', 'وب', 'فا', 'ول', 'وو', 'أف', 'في', 'لا', 'ات', 'وي', 'وت', 'سي', 'سن', 'وا', 'فا', 'وي', 'اا', 'مم', 'كت', 'مت', 'مس', 'وست', 'بمس',\n",
    "'ل', 'ب', 'ف', 'س', 'و', 'ي', 'ت', 'ن', 'ا','م']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def arabic_stemming(text):\n",
    "    prefixes3 = ['ال', 'وال', 'فال', 'بال', 'كال', 'لل']\n",
    "    prefixes4 = ['من', 'ومن', 'في', 'وفي', 'ب', 'وب', 'ل', 'ول', 'س', 'وس', 'ي', 'وي', 'ت', 'وت', 'ن', 'ون']\n",
    "    prefixes4_special = ['و']\n",
    "    suffixes1 = ['ة', 'ه', 'ي', 'ك', 'ها', 'يا', 'نا', 'كم', 'هم', 'هن']\n",
    "    suffixes2 = ['ين', 'ات', 'ون', 'ان', 'تي', 'ته', 'تك', 'ني', 'تن', 'تم', 'ها', 'كما']\n",
    "    \n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "\n",
    "        if len(token) >= 5 and token[:3] in prefixes3:\n",
    "            token = token[3:]\n",
    "        elif len(token) >= 4 and token[:2] in prefixes4:\n",
    "            token = token[2:]\n",
    "        elif len(token) >= 4 and token[0] in prefixes4_special:\n",
    "            token = token[1:]\n",
    "        elif len(token) >= 4 and token[0] in ['ب', 'ل']:\n",
    "            stemmed_token = token[1:]\n",
    "            if stemmed_token not in tokens:\n",
    "                token = stemmed_token\n",
    "        while len(token) >= 4:\n",
    "            if token.endswith(\"ا\"):\n",
    "              token = token[:-2]\n",
    "            elif token[-2:] in suffixes2:\n",
    "                token = token[:-2]\n",
    "            elif token[-1] in suffixes1:\n",
    "                token = token[:-1]\n",
    "            else:\n",
    "                break\n",
    "        while len(token) >= 3 and token[-1] in suffixes1:\n",
    "            token = token[:-1]\n",
    "        stemmed_tokens.append(token)\n",
    "    \n",
    "    \n",
    "    updated_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return updated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= arabic_stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_stopwords(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>إيم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أثر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>صح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دائ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>فخو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>كعلم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>جامع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>تقاليد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>دنيو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>دي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ملتزم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>تنوير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>دوام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>تقاليد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>جود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>شك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>اضح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>بكل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>أشكال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>تد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>يصف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>يرنارد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>غروم</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Words\n",
       "0      إيم\n",
       "1      أثر\n",
       "2       صح\n",
       "3      دائ\n",
       "4      فخو\n",
       "5     كعلم\n",
       "6     جامع\n",
       "7   تقاليد\n",
       "8     دنيو\n",
       "9       دي\n",
       "10   ملتزم\n",
       "11   تنوير\n",
       "12    دوام\n",
       "13  تقاليد\n",
       "14     جود\n",
       "15      شك\n",
       "16     اضح\n",
       "17     بكل\n",
       "18   أشكال\n",
       "19      تد\n",
       "20     يصف\n",
       "21  يرنارد\n",
       "22    غروم"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok= word_tokenize(text)\n",
    "df = pd.DataFrame(tok, columns=['Words'])\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
