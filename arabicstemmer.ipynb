{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pyarabic.araby as araby\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "------------\n",
    "- Readthe document /query text.\n",
    "- Create an Indices Vector.\n",
    "- Normalize text (Part 1 ): remove shaddah and tanween\n",
    "- Remove Stop Words in their different forms ( Prefix + Suffix, Stop Word, Prefix + Stop\n",
    "Word, Stop Word + Suffix, Prefix + Stop Word + Suffix\n",
    "- Normalize text (Part 2 ) : unification of letters ’ forms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"الإيمانُ ..وأثره على الصحة النفسية \n",
    "كان علم النفس دائماً فخوراً كعلم بالجامعات بتقاليده الدنيوية )غير\n",
    "الدينية( الملتزمة بالتنوير. وكان -على الدوام- من ضمن هذه\n",
    "التقاليد وجود شك واضح بكل أشكال التدين  كما يصف بيرنارد\n",
    "وغروم\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe \n",
    "data = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an Indices Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'الإيمانُ': 0, '..': 1, 'وأثره': 2, 'على': 3, 'الصحة': 4, 'النفسية': 5, 'كان': 6, 'علم': 7, 'النفس': 8, 'دائماً': 9, 'فخوراً': 10, 'كعلم': 11, 'بالجامعات': 12, 'بتقاليده': 13, 'الدنيوية': 14, ')': 15, 'غير': 16, 'الدينية': 17, '(': 18, 'الملتزمة': 19, 'بالتنوير': 20, '.': 21, 'وكان': 22, '-على': 23, 'الدوام-': 24, 'من': 25, 'ضمن': 26, 'هذه': 27, 'التقاليد': 28, 'وجود': 29, 'شك': 30, 'واضح': 31, 'بكل': 32, 'أشكال': 33, 'التدين': 34, 'كما': 35, 'يصف': 36, 'بيرنارد': 37, 'وغروم': 38}\n"
     ]
    }
   ],
   "source": [
    "indices_vector = {}\n",
    "tokens = word_tokenize(text)\n",
    "for i, token in enumerate(tokens):\n",
    "    indices_vector[token] = i\n",
    "    \n",
    "print(indices_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3:  Normalize text (Part 1) - remove shaddah and tanween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize1(text): #remove shaddah and tanween\n",
    "    #text = araby.strip_tashkeel(text)\n",
    "    tokens = araby.tokenize(text, conditions=araby.is_arabicrange, morphs=araby.strip_tashkeel)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens  = Normalize1(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \n",
    "    # Read stop words from file\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stop_words = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    #tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = remove_stopwords(tokens)\n",
    "data['Tokens'] = tokens.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Normalize text (Part 2 ) : unification of letters ’ forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Dictionary\n",
    "Norm =  {   \n",
    "            'ا' : ['أ','آ','ء'],\n",
    "            'ي' : ['ى'],\n",
    "            'ه': ['ة' ],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize2(tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == 'الله':\n",
    "            continue\n",
    "        \n",
    "        for key, values in Norm.items():\n",
    "            for value in values:\n",
    "                if value == 'ه' and tokens[i].endswith(value):\n",
    "                    tokens[i] = tokens[i][:-1] + key\n",
    "                elif value in tokens[i]:\n",
    "                    tokens[i] = tokens[i].replace(value, key)\n",
    "                    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Normalize2(tokens)\n",
    "data['Normalized Tokens'] = tokens.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['ناكمو', 'ناهما', 'كموها','موهم', 'موهن', 'ناكم', 'اءكم', 'اءنا', 'اءهم', 'نوهن', 'اؤهم', 'ونهم', 'ائهم', 'ناهم', 'ونكم', 'ونهم', 'ائنا', 'توهن', 'اتها', 'اتهم', 'ياته',\n",
    "'تنا', 'نها', 'تان', 'ونه', 'اءه', 'ناه', 'هما', 'وها', 'نهم', 'وهم', 'وني', 'وهن', 'تها', 'تهم', 'نكم', 'هات', 'تان', 'تهن', 'وكم', 'ونه', 'ونك', 'انك', 'ائك', 'يهم',\n",
    "'ان', 'ين', 'ون', 'نا', 'تم', 'تا', 'ون', 'وا', 'ات', 'ان', 'ين', 'تن', 'كم', 'هن', 'نا', 'يا', 'ها', 'تم', 'كن', 'ني', 'وا', 'ما', 'هم', 'ها', 'وك', 'وت', 'وة', 'ية',\n",
    "'ه', 'ي', 'ك', 'ت', 'ا', 'ن']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_suffix_rules(word):\n",
    "   for suffix in suffixes:\n",
    "        if word.endswith(suffix) and (len(word) - len(suffix)) >= 3:\n",
    "            if suffix == \"ات\":\n",
    "                if not word.startswith(\"ال\") or len(word) > 5:\n",
    "                    word = word[:-2] + \"ه\"\n",
    "                    break\n",
    "                else:\n",
    "                    word = word[:-2]\n",
    "                    break\n",
    "            elif suffix == \"اه\":\n",
    "                word = word[:-2] + \"ي\"\n",
    "                break\n",
    "            elif word.startswith(\"ال\") and suffix not in [\"ه\", \"ي\"] or (len(word) - len(suffix)) <= 5:\n",
    "                return word\n",
    "\n",
    "            else:\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "   return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_allsuffixes(tokens):\n",
    "    new_words = []\n",
    "    for token in tokens:\n",
    "      new_words.append(apply_suffix_rules(token))        \n",
    "    # Join the words back into a text.\n",
    "\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = remove_allsuffixes(tokens)\n",
    "data['Suffex Removed'] = tokens.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prefixes =  ['وليست', 'فليست', 'اليست', 'افلست',\n",
    "'وبال', 'فبال', 'لبال','بال', 'فال', 'وال', 'كال', 'ولل','بال','اتست', 'ويست', 'فاست', 'واست', 'انهم', 'والم', 'باست', 'الاس', 'كمست', 'والا', 'فاست',\n",
    "'مست', 'ولت', 'فلي', 'فلن', 'فلل', 'فان', 'يست', 'است', 'تست', 'فلي', 'وسي', 'وسن', 'فلا', 'وست', 'بمس',\n",
    "'لل', 'ال', 'اا', 'لي','او', 'اي', 'ان', 'في', 'فب', 'فت', 'لي', 'فن', 'وب', 'فا', 'ول', 'وو', 'أف', 'في', 'لا', 'ات', 'وي', 'وت', 'سي', 'سن', 'فا', 'وي', 'مم', 'كت', 'مت', 'مس', 'وست', 'بمس',\n",
    "'ك','ل', 'ب', 'ف', 'س', 'و', 'ي', 'ت', 'ن', 'ا','م']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(word):\n",
    "    if word == \"الله\":\n",
    "        return word\n",
    "    elif word.startswith(\"اا\"):\n",
    "        return word[1:]\n",
    "    elif len(word) >= 5 and word.startswith(\"وال\"):\n",
    "        return word[3:]\n",
    "    elif len(word) > 4 and word.startswith(\"لل\"):\n",
    "        return word[2:]\n",
    "    elif len(word) >= 4 and word.startswith(\"ال\"):\n",
    "        return word[2:]\n",
    "    elif len(word) >= 3:  \n",
    "        for pre in Prefixes:\n",
    "          if word.startswith(pre):\n",
    "              x = int(len(pre))\n",
    "              return word[x:]\n",
    "    return word\n",
    "\n",
    "def remove_allprefixes(tokens):\n",
    "    pref = []\n",
    "    for worde in tokens:\n",
    "       pref.append(remove_prefix(worde))        \n",
    "    # Join the words back into a text. \n",
    "    return pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = remove_allprefixes(tokens)\n",
    "data['Prefex Removed'] = tokens.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Last Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLastSuffix(tokens):\n",
    "    new_words = []\n",
    "    for word in tokens:\n",
    "        if word.endswith(\"ا\"):\n",
    "            new_words.append(word[:-1])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = removeLastSuffix(tokens)\n",
    "data['Last Suffex Removed'] = tokens.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwordss(tokens):\n",
    "    \n",
    "    # Read stop words from file\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stop_words = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    #tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "        else:\n",
    "            filtered_tokens.append(\" - \")\n",
    "            \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenss = remove_stopwordss(tokens)\n",
    "tokens = remove_stopwords(tokens)\n",
    "\n",
    "data['Final Result'] = tokenss.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltext = ' '.join(tokens)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Text Before Stemming\n",
      "--------------------------------------\n",
      "الإيمانُ ..وأثره على الصحة النفسية \n",
      "كان علم النفس دائماً فخوراً كعلم بالجامعات بتقاليده الدنيوية )غير\n",
      "الدينية( الملتزمة بالتنوير. وكان -على الدوام- من ضمن هذه\n",
      "التقاليد وجود شك واضح بكل أشكال التدين  كما يصف بيرنارد\n",
      "وغروم\n",
      "--------------------------------------\n",
      "Text After Stemming\n",
      "--------------------------------------\n",
      "إيمان اثره صحه علم خور علم جامعه تقاليد دنيوي ديني ملتزم تنوير دوام تقاليد جود شك اضح شكال تدين صف يرنارد غروم\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------\")\n",
    "print(\"Text Before Stemming\")\n",
    "print(\"--------------------------------------\")\n",
    "print(text)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Text After Stemming\")\n",
    "print(\"--------------------------------------\")\n",
    "print(finaltext)\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Normalized Tokens</th>\n",
       "      <th>Suffex Removed</th>\n",
       "      <th>Prefex Removed</th>\n",
       "      <th>Last Suffex Removed</th>\n",
       "      <th>Final Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الإيمان</td>\n",
       "      <td>الإيمان</td>\n",
       "      <td>الإيمان</td>\n",
       "      <td>إيمان</td>\n",
       "      <td>إيمان</td>\n",
       "      <td>إيمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وأثره</td>\n",
       "      <td>واثره</td>\n",
       "      <td>واثره</td>\n",
       "      <td>اثره</td>\n",
       "      <td>اثره</td>\n",
       "      <td>اثره</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الصحة</td>\n",
       "      <td>الصحه</td>\n",
       "      <td>الصحه</td>\n",
       "      <td>صحه</td>\n",
       "      <td>صحه</td>\n",
       "      <td>صحه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>النفسية</td>\n",
       "      <td>النفسيه</td>\n",
       "      <td>النفسي</td>\n",
       "      <td>نفسي</td>\n",
       "      <td>نفسي</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>النفس</td>\n",
       "      <td>النفس</td>\n",
       "      <td>النفس</td>\n",
       "      <td>نفس</td>\n",
       "      <td>نفس</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>دائما</td>\n",
       "      <td>دائما</td>\n",
       "      <td>دائما</td>\n",
       "      <td>دائما</td>\n",
       "      <td>دائم</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>فخورا</td>\n",
       "      <td>فخورا</td>\n",
       "      <td>فخورا</td>\n",
       "      <td>خورا</td>\n",
       "      <td>خور</td>\n",
       "      <td>خور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>كعلم</td>\n",
       "      <td>كعلم</td>\n",
       "      <td>كعلم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "      <td>علم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>بالجامعات</td>\n",
       "      <td>بالجامعات</td>\n",
       "      <td>بالجامعه</td>\n",
       "      <td>جامعه</td>\n",
       "      <td>جامعه</td>\n",
       "      <td>جامعه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>بتقاليده</td>\n",
       "      <td>بتقاليده</td>\n",
       "      <td>بتقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>الدنيوية</td>\n",
       "      <td>الدنيويه</td>\n",
       "      <td>الدنيوي</td>\n",
       "      <td>دنيوي</td>\n",
       "      <td>دنيوي</td>\n",
       "      <td>دنيوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>الدينية</td>\n",
       "      <td>الدينيه</td>\n",
       "      <td>الديني</td>\n",
       "      <td>ديني</td>\n",
       "      <td>ديني</td>\n",
       "      <td>ديني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>الملتزمة</td>\n",
       "      <td>الملتزمه</td>\n",
       "      <td>الملتزم</td>\n",
       "      <td>ملتزم</td>\n",
       "      <td>ملتزم</td>\n",
       "      <td>ملتزم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>بالتنوير</td>\n",
       "      <td>بالتنوير</td>\n",
       "      <td>بالتنوير</td>\n",
       "      <td>تنوير</td>\n",
       "      <td>تنوير</td>\n",
       "      <td>تنوير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>الدوام</td>\n",
       "      <td>الدوام</td>\n",
       "      <td>الدوام</td>\n",
       "      <td>دوام</td>\n",
       "      <td>دوام</td>\n",
       "      <td>دوام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>التقاليد</td>\n",
       "      <td>التقاليد</td>\n",
       "      <td>التقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "      <td>تقاليد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>وجود</td>\n",
       "      <td>وجود</td>\n",
       "      <td>وجود</td>\n",
       "      <td>جود</td>\n",
       "      <td>جود</td>\n",
       "      <td>جود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>شك</td>\n",
       "      <td>شك</td>\n",
       "      <td>شك</td>\n",
       "      <td>شك</td>\n",
       "      <td>شك</td>\n",
       "      <td>شك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>واضح</td>\n",
       "      <td>واضح</td>\n",
       "      <td>واضح</td>\n",
       "      <td>اضح</td>\n",
       "      <td>اضح</td>\n",
       "      <td>اضح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>بكل</td>\n",
       "      <td>بكل</td>\n",
       "      <td>بكل</td>\n",
       "      <td>كل</td>\n",
       "      <td>كل</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>أشكال</td>\n",
       "      <td>اشكال</td>\n",
       "      <td>اشكال</td>\n",
       "      <td>شكال</td>\n",
       "      <td>شكال</td>\n",
       "      <td>شكال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>التدين</td>\n",
       "      <td>التدين</td>\n",
       "      <td>التدين</td>\n",
       "      <td>تدين</td>\n",
       "      <td>تدين</td>\n",
       "      <td>تدين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>يصف</td>\n",
       "      <td>يصف</td>\n",
       "      <td>يصف</td>\n",
       "      <td>صف</td>\n",
       "      <td>صف</td>\n",
       "      <td>صف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>بيرنارد</td>\n",
       "      <td>بيرنارد</td>\n",
       "      <td>بيرنارد</td>\n",
       "      <td>يرنارد</td>\n",
       "      <td>يرنارد</td>\n",
       "      <td>يرنارد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>وغروم</td>\n",
       "      <td>وغروم</td>\n",
       "      <td>وغروم</td>\n",
       "      <td>غروم</td>\n",
       "      <td>غروم</td>\n",
       "      <td>غروم</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tokens Normalized Tokens Suffex Removed Prefex Removed  \\\n",
       "0     الإيمان           الإيمان        الإيمان          إيمان   \n",
       "1       وأثره             واثره          واثره           اثره   \n",
       "2       الصحة             الصحه          الصحه            صحه   \n",
       "3     النفسية           النفسيه         النفسي           نفسي   \n",
       "4         علم               علم            علم            علم   \n",
       "5       النفس             النفس          النفس            نفس   \n",
       "6       دائما             دائما          دائما          دائما   \n",
       "7       فخورا             فخورا          فخورا           خورا   \n",
       "8        كعلم              كعلم           كعلم            علم   \n",
       "9   بالجامعات         بالجامعات       بالجامعه          جامعه   \n",
       "10   بتقاليده          بتقاليده        بتقاليد         تقاليد   \n",
       "11   الدنيوية          الدنيويه        الدنيوي          دنيوي   \n",
       "12    الدينية           الدينيه         الديني           ديني   \n",
       "13   الملتزمة          الملتزمه        الملتزم          ملتزم   \n",
       "14   بالتنوير          بالتنوير       بالتنوير          تنوير   \n",
       "15     الدوام            الدوام         الدوام           دوام   \n",
       "16   التقاليد          التقاليد       التقاليد         تقاليد   \n",
       "17       وجود              وجود           وجود            جود   \n",
       "18         شك                شك             شك             شك   \n",
       "19       واضح              واضح           واضح            اضح   \n",
       "20        بكل               بكل            بكل             كل   \n",
       "21      أشكال             اشكال          اشكال           شكال   \n",
       "22     التدين            التدين         التدين           تدين   \n",
       "23        يصف               يصف            يصف             صف   \n",
       "24    بيرنارد           بيرنارد        بيرنارد         يرنارد   \n",
       "25      وغروم             وغروم          وغروم           غروم   \n",
       "\n",
       "   Last Suffex Removed Final Result  \n",
       "0                إيمان        إيمان  \n",
       "1                 اثره         اثره  \n",
       "2                  صحه          صحه  \n",
       "3                 نفسي           -   \n",
       "4                  علم          علم  \n",
       "5                  نفس           -   \n",
       "6                 دائم           -   \n",
       "7                  خور          خور  \n",
       "8                  علم          علم  \n",
       "9                جامعه        جامعه  \n",
       "10              تقاليد       تقاليد  \n",
       "11               دنيوي        دنيوي  \n",
       "12                ديني         ديني  \n",
       "13               ملتزم        ملتزم  \n",
       "14               تنوير        تنوير  \n",
       "15                دوام         دوام  \n",
       "16              تقاليد       تقاليد  \n",
       "17                 جود          جود  \n",
       "18                  شك           شك  \n",
       "19                 اضح          اضح  \n",
       "20                  كل           -   \n",
       "21                شكال         شكال  \n",
       "22                تدين         تدين  \n",
       "23                  صف           صف  \n",
       "24              يرنارد       يرنارد  \n",
       "25                غروم         غروم  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
